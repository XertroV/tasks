---
id: P2.M7.E1.T001
title: Create Tokenizer module
status: done
estimate_hours: 1.5
complexity: low
priority: high
depends_on: []
tags:
- tokenizer
- llm
- cost-tracking
claimed_by: cli-user
claimed_at: '2026-02-06T02:41:46.464155'
started_at: '2026-02-06T02:41:46.464155'
completed_at: '2026-02-06T02:41:47.096734'
duration_minutes: 0.010542816666666668
---

# Create Tokenizer module

Implement the core Tokenizer module for counting tokens in text for different LLM models.

## Requirements

- [x] Create `lib/pag_server/context/tokenizer.ex` module
- [x] Implement `count_tokens/2` function (text, model)
- [x] Implement `estimate_tokens/1` function (rough estimate without model)
- [x] Use approximate tokenization (4 chars ≈ 1 token for estimation)
- [x] Add @doc and @spec for all public functions
- [x] Follow architecture.md:273 module structure

## Acceptance Criteria

- [x] `Tokenizer.count_tokens("hello world", "claude-sonnet-4")` returns accurate count
- [x] `Tokenizer.estimate_tokens("hello world")` returns rough token count
- [x] Module compiles without warnings
- [x] Docstrings explain parameters and return values
- [x] Module is under 200 LoC (target ~150 LoC per architecture.md)

## Implementation Details

```elixir
defmodule PagServer.Context.Tokenizer do
  @moduledoc """
  Token counting for LLM cost estimation.
  
  Provides accurate token counts for different model families
  and rough estimates for context management.
  """

  @doc """
  Count tokens in text for a specific model.
  
  Returns accurate token count based on model's tokenizer.
  Uses approximate counting (4 chars ≈ 1 token) until
  native tokenizers are integrated.
  """
  @spec count_tokens(String.t(), String.t()) :: non_neg_integer()
  def count_tokens(text, model) do
    # Approximate: 4 chars per token
    # TODO: Use native tokenizers when available
    text
    |> String.length()
    |> div(4)
    |> max(1)
  end

  @doc """
  Estimate tokens without model-specific tokenization.
  
  Uses conservative heuristics for quick estimation.
  """
  @spec estimate_tokens(String.t()) :: non_neg_integer()
  def estimate_tokens(text) do
    count_tokens(text, "default")
  end
end
```

## Context

**Plan References**:
- `.plan/2026-02-05-velvet-cascade/architecture.md:273` (Tokenizer module)
- `.plan/2026-02-05-velvet-cascade/testing.md:65-77` (Token counting tests)

**Key Points**:
- Start with approximate counting (4 chars/token)
- Future enhancement: integrate tiktoken for OpenAI, anthropic tokenizer
- Used by context truncation and cost calculation
- Must handle empty strings and edge cases

## Notes

The initial implementation uses a simple heuristic (4 characters ≈ 1 token) which is accurate enough for most use cases. This will be enhanced in future phases to use native tokenizers:
- OpenAI: tiktoken library
- Anthropic: Claude tokenizer
- Others: Model-specific tokenizers

For now, this approximation provides sufficient accuracy for cost estimation and context management.


## Delegation Instructions

**Delegated to subagent by**: cli-user (primary agent)
**Delegation date**: 2026-02-06 08:48 UTC
**Primary task**: P2.M6.E3.T002 - Implement regex-based parser

**Instructions**:
This task was claimed as part of a multi-task batch. The primary agent (cli-user)
should spawn a subagent to complete this task in parallel.

**Recommended approach**:
1. Use the Task tool with subagent_type to create a dedicated agent
2. Provide context from this task file as the prompt
3. Grant necessary tools for task completion
4. Monitor subagent progress
5. Aggregate results upon completion

**Independence verification**:
- Different epic: ✓ (P2.M7.E1 vs P2.M6.E3)
- No dependency chain: ✓ (verified at claim time)
