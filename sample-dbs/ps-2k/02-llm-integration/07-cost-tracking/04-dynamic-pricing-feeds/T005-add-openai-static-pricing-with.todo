---
id: P2.M7.E4.T005
title: Add OpenAI static pricing with future API hook
status: done
estimate_hours: 1.0
complexity: low
priority: medium
depends_on:
- P2.M7.E4.T002
tags:
- openai
- pricing
- static
claimed_by: cli-user
claimed_at: '2026-02-06T15:06:21.249594'
started_at: '2026-02-06T15:06:21.249594'
completed_at: '2026-02-06T15:13:05.167320'
duration_minutes: 6.7319619
---

# Add OpenAI static pricing with future API hook



## Requirements

- [ ] Create `lib/pag_server/llm/pricing_fetchers/openai.ex` implementing `PricingFetcher` behaviour
- [ ] Implement `fetch_pricing/1`: return static OpenAI pricing with note for future API integration
- [ ] Implement `provider_name/0`: return `:openai`
- [ ] Implement `supports_model?/1`: return true for models starting with "gpt" or "o" (o4-mini, etc.)
- [ ] Use current OpenAI pricing from `LLM.Pricing` module as static baseline
- [ ] Add `@moduledoc` explaining this is static pricing with future API hook
- [ ] Add `TODO` comment in `fetch_pricing/1` for future OpenAI pricing API integration
- [ ] Register the fetcher in `Application.start/2` after supervisor starts
- [ ] Add typespec annotations for all public functions
- [ ] Handle unknown GPT models gracefully with default pricing

## Acceptance Criteria

- [ ] Module created at `lib/pag_server/llm/pricing_fetchers/openai.ex`
- [ ] `OpenAIPricingFetcher.fetch_pricing("gpt-5.2")` returns correct static pricing
- [ ] `OpenAIPricingFetcher.provider_name()` returns `:openai`
- [ ] `OpenAIPricingFetcher.supports_model?("gpt-4.1")` returns `true`
- [ ] `OpenAIPricingFetcher.supports_model?("o4-mini")` returns `true`
- [ ] `OpenAIPricingFetcher.supports_model?("claude-sonnet-4.5")` returns `false`
- [ ] Pricing matches current OpenAI rates:
  - [ ] GPT-5.2: $1.75/$14.00 per million tokens
  - [ ] GPT-5.2 Pro: $21.00/$168.00 per million tokens
  - [ ] GPT-5 Mini: $0.25/$2.00 per million tokens
  - [ ] GPT-4.1: $3.00/$12.00 per million tokens
  - [ ] GPT-4.1 Mini: $0.80/$3.20 per million tokens
  - [ ] o4-mini: $4.00/$16.00 per million tokens
- [ ] `cached_per_million` calculated as 10% of `prompt_per_million`
- [ ] Unknown GPT models return default pricing ($3.00/$12.00/$0.30)
- [ ] `PricingFetcher.get_pricing("gpt-5.2")` uses OpenAI fetcher
- [ ] Fetcher registered during application startup
- [ ] No compiler warnings
- [ ] All functions have `@spec` declarations

## Implementation Notes

### Static Pricing Map

```elixir
defmodule PagServer.LLM.PricingFetchers.OpenAI do
  @behaviour PagServer.LLM.PricingFetcher

  @moduledoc """
  Static pricing fetcher for OpenAI GPT models.
  
  This implementation uses hardcoded pricing as a baseline.
  Future enhancement: integrate with OpenAI's models API pricing data.
  
  ## Pricing Source
  https://openai.com/api/pricing/ (verified Feb 2026)
  """

  @pricing %{
    # Latest frontier models (Feb 2026)
    "gpt-5.2" => %{
      prompt_per_million: 1.75,
      completion_per_million: 14.0,
      cached_per_million: 0.175
    },
    "gpt-5.2-pro" => %{
      prompt_per_million: 21.0,
      completion_per_million: 168.0,
      cached_per_million: 2.1
    },
    "gpt-5-mini" => %{
      prompt_per_million: 0.25,
      completion_per_million: 2.0,
      cached_per_million: 0.025
    },
    # Fine-tuning and specialized models
    "gpt-4.1" => %{
      prompt_per_million: 3.0,
      completion_per_million: 12.0,
      cached_per_million: 0.3
    },
    "gpt-4.1-mini" => %{
      prompt_per_million: 0.80,
      completion_per_million: 3.20,
      cached_per_million: 0.08
    },
    "gpt-4.1-nano" => %{
      prompt_per_million: 0.20,
      completion_per_million: 0.80,
      cached_per_million: 0.02
    },
    # Reasoning models
    "o4-mini" => %{
      prompt_per_million: 4.0,
      completion_per_million: 16.0,
      cached_per_million: 0.4
    },
    # Realtime API models
    "gpt-realtime" => %{
      prompt_per_million: 4.0,
      completion_per_million: 16.0,
      cached_per_million: 0.4
    },
    "gpt-realtime-mini" => %{
      prompt_per_million: 0.60,
      completion_per_million: 2.40,
      cached_per_million: 0.06
    }
  }

  @default_pricing %{
    prompt_per_million: 3.0,
    completion_per_million: 12.0,
    cached_per_million: 0.3
  }

  @impl true
  def provider_name, do: :openai

  @impl true
  def supports_model?(model) when is_binary(model) do
    normalized = String.downcase(model)
    String.starts_with?(normalized, "gpt") or String.starts_with?(normalized, "o")
  end

  @impl true
  def fetch_pricing(model) when is_binary(model) do
    # TODO: Integrate with OpenAI models API for dynamic pricing
    # Endpoint: GET https://api.openai.com/v1/models/{model}
    # Response includes pricing data in model.pricing field
    
    normalized = normalize_model(model)
    pricing = Map.get(@pricing, normalized, @default_pricing)
    {:ok, pricing}
  end

  defp normalize_model(model) do
    model
    |> String.downcase()
    |> String.trim()
  end
end
```

### Registration in Application

Add to `register_llm_providers/0` in `lib/pag_server/application.ex`:

```elixir
defp register_llm_providers do
  # ... existing provider registrations ...
  
  # Register pricing fetchers
  PagServer.LLM.PricingFetcher.register(:anthropic, PagServer.LLM.PricingFetchers.Anthropic)
  PagServer.LLM.PricingFetcher.register(:openai, PagServer.LLM.PricingFetchers.OpenAI)
  
  :ok
end
```

## Context

**Dependencies:**
- T002 (done): PricingFetcher behaviour and registry established

**Future Enhancement:**
OpenAI's `/v1/models` API endpoint returns model metadata but does not currently include pricing information in the response. This implementation uses static pricing as verified from OpenAI's pricing page. When OpenAI adds pricing data to their API responses, this module should be updated to fetch live data while falling back to static pricing on API failures.

**Reference Files:**
- `lib/pag_server/llm/pricing_fetcher.ex` - Behaviour definition
- `lib/pag_server/llm/pricing.ex` - Current static pricing source
- `lib/pag_server/application.ex` - Registration location

## Notes

- OpenAI's reasoning models (o4-mini) use the same pricing structure but may have different token counting (includes reasoning tokens)
- Realtime API models have separate audio pricing not covered here
- The `supports_model?/1` implementation uses prefix matching which covers both "gpt-" and "o" (for o4-mini)
