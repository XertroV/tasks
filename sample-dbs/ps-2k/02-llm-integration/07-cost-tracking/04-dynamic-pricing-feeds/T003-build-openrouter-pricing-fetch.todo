---
id: P2.M7.E4.T003
title: Build OpenRouter pricing fetcher
status: done
estimate_hours: 2.0
complexity: medium
priority: high
depends_on:
- P2.M7.E4.T002
tags:
- openrouter
- api
- pricing
claimed_by: cli-user
claimed_at: '2026-02-06T15:06:21.247093'
started_at: '2026-02-06T15:06:21.247093'
completed_at: '2026-02-06T15:13:03.673851'
duration_minutes: 6.7071124499999994
---

# Build OpenRouter pricing fetcher

Implement the first concrete `PagServer.LLM.PricingFetcher` implementation,
fetching live model pricing from OpenRouter's public `/api/v1/models` endpoint.
The PricingFetcher behaviour and registry already exist (T002, done) but have
no implementations yet.

## Requirements

- [ ] Create `lib/pag_server/llm/pricing_fetchers/openrouter.ex` implementing `PricingFetcher` behaviour
- [ ] Implement `fetch_pricing/1`: query OpenRouter `/api/v1/models` endpoint for model pricing
- [ ] Implement `provider_name/0`: return `:openrouter`
- [ ] Implement `supports_model?/1`: return true for models with `openrouter` prefix or `/` separator
- [ ] Parse OpenRouter model response to extract `pricing.prompt`, `pricing.completion` fields
- [ ] Convert OpenRouter's per-token pricing to per-million-token format used internally
- [ ] Handle API errors gracefully (network failure, unexpected response format)
- [ ] Cache the full model list response to avoid repeated API calls for individual model lookups
- [ ] Provide `list_supported_models/0` callback returning all models from cached OpenRouter API response
- [ ] Implement GenServer to maintain in-memory cache of full model list (refreshed on demand)
- [ ] Add rate limiting protection: OpenRouter has API usage limits, implement backoff on 429 responses
- [ ] Add request timeout (10 seconds)
- [ ] Use `Req` HTTP client consistent with other provider implementations
- [ ] Register the fetcher during application startup in `Application.start/2`
- [ ] Add `@moduledoc` with usage documentation

## Acceptance Criteria

- [ ] `OpenRouterPricingFetcher.fetch_pricing("anthropic/claude-sonnet-4")` returns correct pricing
- [ ] `OpenRouterPricingFetcher.list_supported_models()` returns full list of available models from OpenRouter
- [ ] Model list is cached in GenServer state and reused across calls
- [ ] Pricing values match OpenRouter's published rates
- [ ] Network errors return `{:error, reason}` (not crash)
- [ ] Malformed API responses return `{:error, :parse_error}` (not crash)
- [ ] Rate limit (429) responses trigger exponential backoff and return `{:error, :rate_limited}`
- [ ] `PricingFetcher.get_pricing("openrouter-model")` finds and uses this fetcher
- [ ] Fallback to static pricing works when OpenRouter API is unavailable

## Implementation Details

OpenRouter's `/api/v1/models` returns a JSON array of model objects:

```json
{
  "data": [
    {
      "id": "anthropic/claude-sonnet-4",
      "pricing": {
        "prompt": "0.000003",     // per token
        "completion": "0.000015"  // per token
      },
      "context_length": 200000,
      ...
    }
  ]
}
```

### Model Discovery Strategy

Implement as a GenServer to cache the full model list in memory:

1. **On startup**: Fetch full model list from `/api/v1/models` and cache in state
2. **`fetch_pricing/1`**: Look up model in cached list (avoid per-model API calls)
3. **`list_supported_models/0`**: Return cached model IDs
4. **Refresh on demand**: Expose `refresh_models/0` to force re-fetch from API
5. **Rate limiting**: Track 429 responses, implement exponential backoff (1s, 2s, 4s, 8s...)

Implementation:

```elixir
defmodule PagServer.LLM.PricingFetchers.OpenRouter do
  @behaviour PagServer.LLM.PricingFetcher
  use GenServer

  @base_url "https://openrouter.ai"
  @models_endpoint "/api/v1/models"

  # Client API

  def start_link(opts \\ []) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  @impl true
  def provider_name, do: :openrouter

  @impl true
  def supports_model?(model) do
    String.contains?(model, "/")
  end

  @impl true
  def fetch_pricing(model) do
    GenServer.call(__MODULE__, {:fetch_pricing, model})
  end

  def list_supported_models do
    GenServer.call(__MODULE__, :list_models)
  end

  def refresh_models do
    GenServer.call(__MODULE__, :refresh_models)
  end

  # Server callbacks

  @impl true
  def init(_opts) do
    case fetch_models_from_api() do
      {:ok, models} ->
        {:ok, %{models: models, last_fetch: System.monotonic_time(:millisecond), backoff: 0}}

      {:error, _reason} ->
        # Start with empty cache, retry later
        {:ok, %{models: [], last_fetch: nil, backoff: 0}}
    end
  end

  @impl true
  def handle_call({:fetch_pricing, model}, _from, state) do
    case find_model(state.models, model) do
      {:ok, model_data} ->
        {:reply, parse_pricing(model_data), state}

      {:error, _} = error ->
        {:reply, error, state}
    end
  end

  @impl true
  def handle_call(:list_models, _from, state) do
    model_ids = Enum.map(state.models, & &1["id"])
    {:reply, {:ok, model_ids}, state}
  end

  @impl true
  def handle_call(:refresh_models, _from, state) do
    case fetch_models_from_api() do
      {:ok, models} ->
        new_state = %{state | models: models, last_fetch: System.monotonic_time(:millisecond), backoff: 0}
        {:reply, :ok, new_state}

      {:error, :rate_limited} ->
        # Exponential backoff: 1s, 2s, 4s, 8s (capped at 8s)
        backoff = min(state.backoff + 1, 3)
        Process.sleep(:math.pow(2, backoff) * 1000)
        {:reply, {:error, :rate_limited}, %{state | backoff: backoff}}

      {:error, reason} ->
        {:reply, {:error, reason}, state}
    end
  end

  # Private helpers

  defp fetch_models_from_api do
    req = Req.new(base_url: @base_url, receive_timeout: 10_000)

    case Req.get(req, url: @models_endpoint) do
      {:ok, %{status: 200, body: %{"data" => data}}} ->
        {:ok, data}

      {:ok, %{status: 429}} ->
        {:error, :rate_limited}

      {:ok, %{status: status}} ->
        {:error, {:http_error, status}}

      {:error, reason} ->
        {:error, {:connection_error, reason}}
    end
  end

  defp find_model(models, model_id) do
    case Enum.find(models, fn m -> m["id"] == model_id end) do
      nil -> {:error, {:model_not_found, model_id}}
      model -> {:ok, model}
    end
  end

  defp parse_pricing(%{"pricing" => %{"prompt" => prompt, "completion" => completion}}) do
    # Convert per-token to per-million-token
    {:ok, %{
      prompt_per_million: parse_price(prompt) * 1_000_000,
      completion_per_million: parse_price(completion) * 1_000_000,
      cached_per_million: 0.0
    }}
  end

  defp parse_pricing(_), do: {:error, :parse_error}

  defp parse_price(price) when is_binary(price), do: String.to_float(price)
  defp parse_price(price) when is_number(price), do: price
end
```

## Context

**Source**: Architecture audit - LLM provider layer gap analysis (2026-02-06)

**Depends On**:
- P2.M7.E4.T002 (done): PricingFetcher behaviour and registry

**Reference Files**:
- `lib/pag_server/llm/pricing_fetcher.ex` - Behaviour definition and ETS registry
- `lib/pag_server/llm/providers/openrouter.ex` - OpenRouter provider (has static pricing)
- `lib/pag_server/llm/providers/openrouter/client.ex` - OpenRouter HTTP client (if exists)

## Notes

- OpenRouter's `/api/v1/models` is a public endpoint that does not require authentication
- The response includes all available models (200+ as of 2026), so caching the full list is essential
- **Rate limiting**: OpenRouter enforces rate limits on the `/api/v1/models` endpoint. Respect 429 responses with exponential backoff.
- **Caching strategy**: Maintain an in-memory cache as a GenServer. Avoid repeated API calls for individual model lookups.
- **Model discovery**: The `list_supported_models/0` callback enables other parts of the system to discover which models are available without hardcoding them.
- Price strings may be "0" for free models -- handle `String.to_float("0")` correctly
- Some models may have `null` pricing -- return `{:error, :pricing_unavailable}` for those
- The pricing cache layer (T006) will wrap this fetcher, adding TTL-based caching on top of the in-memory model list cache


## Sibling Batch Instructions

**Batch mode**: siblings (same epic: P2.M7.E4)
**Agent**: cli-user
**Date**: 2026-02-07 02:06 UTC
**Sibling tasks**: P2.M7.E4.T004, P2.M7.E4.T005, P2.M7.E4.T006

**Instructions**:
This task is part of a sibling batch from the same epic.
Spawn ONE subagent to implement ALL sibling tasks sequentially.
Work through tasks in order: P2.M7.E4.T003 → P2.M7.E4.T004 → P2.M7.E4.T005 → P2.M7.E4.T006
Mark each done individually after completion.

**Task files**:
- P2.M7.E4.T003: .tasks/02-llm-integration/07-cost-tracking/04-dynamic-pricing-feeds/T003-build-openrouter-pricing-fetch.todo
- P2.M7.E4.T004: .tasks/02-llm-integration/07-cost-tracking/04-dynamic-pricing-feeds/T004-add-anthropic-static-pricing-w.todo
- P2.M7.E4.T005: .tasks/02-llm-integration/07-cost-tracking/04-dynamic-pricing-feeds/T005-add-openai-static-pricing-with.todo
- P2.M7.E4.T006: .tasks/02-llm-integration/07-cost-tracking/04-dynamic-pricing-feeds/T006-implement-pricing-cache-layer.todo
